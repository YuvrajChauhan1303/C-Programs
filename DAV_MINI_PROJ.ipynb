{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/Room1097/ECG_Feature_Analysis_DAV_Project/blob/main/DAV_MINI_PROJ.ipynb",
      "authorship_tag": "ABX9TyMAClp5qEzOnmo6kH8KJz4S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuvrajChauhan1303/C-Programs/blob/main/DAV_MINI_PROJ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pGnprFAPJ5fa"
      },
      "outputs": [],
      "source": [
        "# STEP 0: Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import skew, kurtosis, entropy\n",
        "from scipy.signal import welch, find_peaks\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "id": "F24ED1L5S-E2",
        "outputId": "60e83d29-7ccb-4588-9ad7-975a3a1f55f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/a01.csv'\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "NdOFY1IuTBxh",
        "outputId": "dd4471d5-3d9f-4b4f-de82-88800799c5c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/a01.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-08b29c3e202f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/a01.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/a01.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "635wyYKxLw22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "w0aPmohcL2D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "UHdgB2McL3-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ones_count = df.iloc[:, -1].value_counts()[1] if 1 in df.iloc[:, -1].value_counts() else 0\n",
        "print(f\"Number of 1s in the last column: {ones_count}\")\n"
      ],
      "metadata": {
        "id": "aDg_tJ6jMsSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_raw = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]"
      ],
      "metadata": {
        "id": "KoElagXTL6Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "    plt.figure(figsize=(12, 3))\n",
        "    plt.plot(X_raw.iloc[i].values[:1000])\n",
        "    plt.title(f\"ECG Signal - Sample {i}\")\n",
        "    plt.xlabel(\"Sample Index\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.show()\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x=y)\n",
        "plt.title(\"Sleep Apnea Label Distribution\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VadldmLfMNIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Feature Extraction\n",
        "\n",
        "def extract_features(signal):\n",
        "    features = {}\n",
        "    features['mean'] = np.mean(signal)\n",
        "    features['std'] = np.std(signal)\n",
        "    features['var'] = np.var(signal)\n",
        "    features['median'] = np.median(signal)\n",
        "    features['min'] = np.min(signal)\n",
        "    features['max'] = np.max(signal)\n",
        "    features['range'] = np.ptp(signal)\n",
        "    features['q1'] = np.percentile(signal, 25)\n",
        "    features['q3'] = np.percentile(signal, 75)\n",
        "    features['iqr'] = features['q3'] - features['q1']\n",
        "    features['skewness'] = skew(signal)\n",
        "    features['kurtosis'] = kurtosis(signal)\n",
        "    features['rms'] = np.sqrt(np.mean(signal**2))\n",
        "    features['zero_crossings'] = np.count_nonzero(np.diff(np.sign(signal)))\n",
        "    abs_diff = np.abs(np.diff(signal))\n",
        "    features['abs_diff_mean'] = np.mean(abs_diff)\n",
        "    features['abs_diff_std'] = np.std(abs_diff)\n",
        "    features['symmetry'] = np.mean(np.abs(signal - signal[::-1]))\n",
        "    features['signal_energy'] = np.sum(signal**2)\n",
        "    hist, _ = np.histogram(signal, bins=100, density=True)\n",
        "    features['signal_entropy'] = entropy(hist + 1e-6)\n",
        "    freqs, psd = welch(signal, fs=250)\n",
        "    total_power = np.sum(psd)\n",
        "    vlf = np.sum(psd[(freqs >= 0.003) & (freqs < 0.04)])\n",
        "    lf = np.sum(psd[(freqs >= 0.04) & (freqs < 0.15)])\n",
        "    hf = np.sum(psd[(freqs >= 0.15) & (freqs < 0.4)])\n",
        "    features['total_power'] = total_power\n",
        "    features['vlf_power'] = vlf\n",
        "    features['lf_power'] = lf\n",
        "    features['hf_power'] = hf\n",
        "    features['lf_hf_ratio'] = lf / (hf + 1e-6)\n",
        "    peaks, _ = find_peaks(signal, distance=50, height=np.mean(signal))\n",
        "    rr_intervals = np.diff(peaks) if len(peaks) > 1 else [0]\n",
        "    features['num_beats'] = len(peaks)\n",
        "    features['mean_rr'] = np.mean(rr_intervals) if len(rr_intervals) > 0 else 0\n",
        "    features['std_rr'] = np.std(rr_intervals) if len(rr_intervals) > 0 else 0\n",
        "    features['min_rr'] = np.min(rr_intervals) if len(rr_intervals) > 0 else 0\n",
        "    features['max_rr'] = np.max(rr_intervals) if len(rr_intervals) > 0 else 0\n",
        "    features['heart_rate'] = (60 / (features['mean_rr'] / 250)) if features['mean_rr'] > 0 else 0\n",
        "    return features\n",
        "\n",
        "# Extract for all samples\n",
        "feature_list = [extract_features(row) for row in X_raw.values]\n",
        "X_features = pd.DataFrame(feature_list)\n"
      ],
      "metadata": {
        "id": "xeuwIfoTM-6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Feature Visualization\n",
        "\n",
        "# Histogram of selected features\n",
        "for feat in ['mean', 'std', 'rms', 'signal_entropy', 'heart_rate']:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.histplot(X_features[feat], bins=30, kde=True)\n",
        "    plt.title(f\"Distribution of {feat}\")\n",
        "    plt.xlabel(feat)\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()\n",
        "\n",
        "# Correlation heatmap\n",
        "plt.figure(figsize=(16, 12))\n",
        "sns.heatmap(X_features.corr(), cmap='coolwarm', center=0)\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HPaVXRDSNKyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold"
      ],
      "metadata": {
        "id": "4usN768jQkIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Remove constant features\n",
        "constant_filter = VarianceThreshold(threshold=0.0)\n",
        "X_no_constants = constant_filter.fit_transform(X_features)\n",
        "filtered_feature_names = X_features.columns[constant_filter.get_support()]\n",
        "\n",
        "# Step 2: Scale the filtered features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_no_constants)\n",
        "\n",
        "# Step 3: Select top K features (adjust k if needed)\n",
        "k = min(15, X_scaled.shape[1])  # Just in case fewer than 15 features remain\n",
        "selector = SelectKBest(score_func=f_classif, k=k)\n",
        "X_selected = selector.fit_transform(X_scaled, y)\n",
        "\n",
        "# Step 4: Get names of selected features\n",
        "selected_mask = selector.get_support()\n",
        "selected_feature_names = filtered_feature_names[selected_mask]\n",
        "\n",
        "print(\"Selected Features:\\n\", list(selected_feature_names))"
      ],
      "metadata": {
        "id": "S4TpJ5U1NN1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Dimensionality Reduction (PCA)\n",
        "\n",
        "pca = PCA(n_components=0.95)  # retain 95% variance\n",
        "X_reduced = pca.fit_transform(X_selected)\n",
        "print(f\"PCA reduced shape: {X_reduced.shape}\")\n"
      ],
      "metadata": {
        "id": "P5cDWjqcNc4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: Resample to Fix Imbalance & Split\n",
        "\n",
        "sm = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = sm.fit_resample(X_reduced, y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_resampled, y_resampled, test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "linFmv3MNfUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 8: Train Classifier\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "aYzoeVkvNhTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 9: Plot Feature Importances for All Selected Features\n",
        "\n",
        "# Get importances from classifier\n",
        "importances = clf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Use correct selected feature names (after VarianceThreshold + SelectKBest)\n",
        "selected_feature_names = filtered_feature_names[selector.get_support()]\n",
        "\n",
        "# Sort selected features and their importances\n",
        "sorted_feature_names = [selected_feature_names[i] for i in indices]\n",
        "sorted_importances = importances[indices]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, len(sorted_feature_names) * 0.4))\n",
        "sns.barplot(x=sorted_importances, y=sorted_feature_names, palette=\"viridis\", hue=sorted_feature_names)\n",
        "plt.title(\"Feature Importances (All Selected Features)\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dc4WKVcwOZaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_hypothesis_tests(X_features: pd.DataFrame, y: pd.Series, top_n=5):\n",
        "    df = X_features.copy()\n",
        "    df['label'] = y.values\n",
        "    results = []\n",
        "    for col in X_features.columns[:top_n]:\n",
        "        group0 = df[df['label'] == 0][col]\n",
        "        group1 = df[df['label'] == 1][col]\n",
        "        try: f_stat, f_p = f_oneway(group0, group1)\n",
        "        except: f_stat, f_p = np.nan, np.nan\n",
        "        try:\n",
        "            binned = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='uniform')\n",
        "            df['binned'] = binned.fit_transform(df[[col]])\n",
        "            contingency = pd.crosstab(df['binned'], df['label'])\n",
        "            chi2_stat, chi2_p, _, _ = chi2_contingency(contingency)\n",
        "        except: chi2_stat, chi2_p = np.nan, np.nan\n",
        "        try: t_stat, t_p = ttest_ind(group0, group1, equal_var=False)\n",
        "        except: t_stat, t_p = np.nan, np.nan\n",
        "        try: u_stat, u_p = mannwhitneyu(group0, group1)\n",
        "        except: u_stat, u_p = np.nan, np.nan\n",
        "        try: r, r_p = pearsonr(df[col], df['label'])\n",
        "        except: r, r_p = np.nan, np.nan\n",
        "        results.append({\n",
        "            'Feature': col,\n",
        "            'ANOVA_F': f_stat, 'ANOVA_p': f_p,\n",
        "            'Chi2': chi2_stat, 'Chi2_p': chi2_p,\n",
        "            'T_Stat': t_stat, 'T_p': t_p,\n",
        "            'U_Stat': u_stat, 'U_p': u_p,\n",
        "            'Pearson_r': r, 'Pearson_p': r_p\n",
        "        })\n",
        "    return pd.DataFrame(results).round(4)\n",
        "\n",
        "# Run tests\n",
        "results_df = perform_hypothesis_tests(X_features, y, top_n=5)\n",
        "display(results_df)\n"
      ],
      "metadata": {
        "id": "a1kNYj3kSH2-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}